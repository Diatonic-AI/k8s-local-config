apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama3-microservices-adapter
  namespace: llama3-multi-adapter
  labels:
    app: llama3-microservices-adapter
    component: adapter
    adapter-type: microservices
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama3-microservices-adapter
  template:
    metadata:
      labels:
        app: llama3-microservices-adapter
        component: adapter
        adapter-type: microservices
    spec:
      initContainers:
      - name: init-microservices-adapter
        image: huggingface/transformers-pytorch-gpu:latest
        command: ["/bin/bash", "-c"]
        args:
          - |
            set -e
            echo "Initializing microservices architecture adapter..."
            
            if [ -f "/adapter-data/adapter_config.json" ]; then
              echo "Microservices adapter already exists"
              exit 0
            fi
            
            mkdir -p /adapter-data
            
            cat > /adapter-data/adapter_config.json << 'ADAPTEREOF'
            {
              "peft_type": "LORA",
              "task_type": "CAUSAL_LM",
              "r": 8,
              "lora_alpha": 16,
              "lora_dropout": 0.05,
              "target_modules": ["q_proj", "v_proj"],
              "bias": "none",
              "inference_mode": false,
              "base_model_name_or_path": "/models/base/llama-3-8b"
            }
            ADAPTEREOF
            
            echo "Microservices adapter initialized successfully"
        volumeMounts:
        - name: adapter-storage
          mountPath: /adapter-data
        - name: base-model
          mountPath: /models/base
          readOnly: true
      
      containers:
      - name: microservices-adapter
        image: huggingface/transformers-pytorch-gpu:latest
        command: ["/bin/bash", "-c"]
        args:
          - |
            set -e
            echo "Starting Microservices Architecture Adapter..."
            
            pip install --no-cache-dir \
              peft==0.7.0 \
              bitsandbytes==0.41.3 \
              fastapi==0.104.1 \
              uvicorn==0.24.0 \
              pydantic==2.5.0
            
            cat > /tmp/serve_microservices_adapter.py << 'PYEOF'
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

app = FastAPI(title="Llama3 Microservices Architecture Adapter API")

class ArchitectureRequest(BaseModel):
    requirements: str
    domain: str = "general"
    scale: str = "medium"
    max_length: int = 4096
    temperature: float = 0.4

class ArchitectureResponse(BaseModel):
    architecture_design: str
    patterns_used: list
    technologies_recommended: list

print("Loading base model...")
base_model_path = "/models/base/llama-3-8b"
adapter_path = "/models/adapters/microservices"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

if os.path.exists(adapter_path):
    print("Loading microservices adapter...")
    model = PeftModel.from_pretrained(model, adapter_path)

tokenizer = AutoTokenizer.from_pretrained(base_model_path)
tokenizer.pad_token = tokenizer.eos_token

print("Microservices architecture advisor ready!")

@app.get("/health")
def health_check():
    return {"status": "healthy", "adapter": "microservices-architecture"}

@app.post("/generate", response_model=ArchitectureResponse)
def design_architecture(request: ArchitectureRequest):
    try:
        system_prompt = """You are a senior software architect. Design a microservices architecture based on the requirements.
        
        Consider:
        - Domain boundaries and service decomposition
        - Data consistency patterns
        - Communication patterns (sync/async)
        - Technology recommendations
        - Scalability and fault tolerance
        - Deployment and monitoring
        """
        
        full_prompt = f"""{system_prompt}
        
Requirements: {request.requirements}
Domain: {request.domain}
Expected Scale: {request.scale}

Architectural Design:
"""
        
        inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=request.max_length,
                temperature=request.temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        design = generated.split("Architectural Design:")[-1].strip()
        
        # Extract patterns and technologies (simplified)
        patterns = ["microservices", "event-driven", "api-gateway"]
        technologies = ["kubernetes", "docker", "service-mesh", "mongodb"]
        
        return ArchitectureResponse(
            architecture_design=design,
            patterns_used=patterns,
            technologies_recommended=technologies
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)
PYEOF
            
            python /tmp/serve_microservices_adapter.py
        ports:
        - name: http
          containerPort: 8080
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:512"
        volumeMounts:
        - name: base-model
          mountPath: /models/base
          readOnly: true
        - name: adapter-storage
          mountPath: /models/adapters/microservices
        - name: config
          mountPath: /config
        resources:
          requests:
            memory: "8Gi"
            cpu: "4"
            nvidia.com/gpu: "1"
          limits:
            memory: "10Gi"
            cpu: "5"
            nvidia.com/gpu: "1"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 120
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
      
      volumes:
      - name: base-model
        persistentVolumeClaim:
          claimName: llama3-base-model
      - name: adapter-storage
        persistentVolumeClaim:
          claimName: llama3-microservices-adapter
      - name: config
        configMap:
          name: llama3-microservices-adapter-config
