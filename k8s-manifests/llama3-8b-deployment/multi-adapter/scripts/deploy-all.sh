#!/bin/bash
set -euo pipefail

BASE_DIR="/home/daclab-ai/k3s-multicloud-config/k3s-manifests/llama3-deployment/multi-adapter"

echo "ğŸš€ Deploying Multi-Adapter Llama3 Architecture"
echo "================================================"
echo ""

# Phase 1: Namespace
echo "ğŸ“¦ Phase 1: Creating namespace..."
kubectl apply -f "$BASE_DIR/namespace.yaml"
echo "âœ… Namespace created"
echo ""

# Phase 2: Storage
echo "ğŸ’¾ Phase 2: Creating storage..."
kubectl apply -f "$BASE_DIR/storage/"
echo "âœ… Storage created"
echo ""
echo "â³ Waiting for PVCs to be bound..."
kubectl wait --for=jsonpath='{.status.phase}'=Bound \
  pvc/llama3-base-models-pvc \
  pvc/llama3-chatbot-adapter-pvc \
  pvc/llama3-code-adapter-pvc \
  pvc/llama3-summarization-adapter-pvc \
  -n llama3-multi-adapter \
  --timeout=60s
echo "âœ… All PVCs bound"
echo ""

# Phase 3: Base Model
echo "ğŸ—ï¸  Phase 3: Deploying base model..."
kubectl apply -f "$BASE_DIR/base/"
echo "âœ… Base model deployment created"
echo ""
echo "â³ Waiting for base model to be ready..."
kubectl wait --for=condition=Available \
  deployment/ollama-base \
  -n llama3-multi-adapter \
  --timeout=300s
echo "âœ… Base model ready"
echo ""

# Phase 4: Adapters
echo "ğŸ¯ Phase 4: Deploying adapters..."
echo "  ğŸ“± Deploying chatbot adapter (3 replicas on GPU 0)..."
kubectl apply -f "$BASE_DIR/adapters/chatbot/"
echo "  ğŸ’» Deploying code adapter (2 replicas on GPU 1)..."
kubectl apply -f "$BASE_DIR/adapters/code/"
echo "  ğŸ“ Deploying summarization adapter (1 replica on GPU 1)..."
kubectl apply -f "$BASE_DIR/adapters/summarization/"
echo "âœ… All adapters deployed"
echo ""

echo "â³ Waiting for adapters to be ready..."
kubectl wait --for=condition=Available \
  deployment/ollama-adapter-chatbot \
  deployment/ollama-adapter-code \
  deployment/ollama-adapter-summarization \
  -n llama3-multi-adapter \
  --timeout=300s
echo "âœ… All adapters ready"
echo ""

# Phase 5: Routing
echo "ğŸ”€ Phase 5: Setting up routing..."
kubectl apply -f "$BASE_DIR/routing/"
echo "âœ… Routing configured"
echo ""

# Status Summary
echo "ğŸ“Š Deployment Summary"
echo "===================="
echo ""
echo "Namespace:"
kubectl get namespace llama3-multi-adapter
echo ""
echo "Storage:"
kubectl get pvc -n llama3-multi-adapter
echo ""
echo "Deployments:"
kubectl get deployments -n llama3-multi-adapter
echo ""
echo "Services:"
kubectl get services -n llama3-multi-adapter
echo ""
echo "Pods:"
kubectl get pods -n llama3-multi-adapter -o wide
echo ""
echo "Ingress:"
kubectl get ingress -n llama3-multi-adapter
echo ""

echo "ğŸ‰ Multi-Adapter Deployment Complete!"
echo ""
echo "ğŸ“‹ Next Steps:"
echo "  1. Load base model: kubectl exec -n llama3-multi-adapter -it deployment/ollama-base -- ollama pull llama3:8b"
echo "  2. Test base endpoint: curl -X POST http://ollama-base.llama3-multi-adapter:11434/api/generate -d '{\"model\":\"llama3:8b\",\"prompt\":\"Hello\"}'"
echo "  3. Load adapters (example): kubectl exec -n llama3-multi-adapter -it deployment/ollama-adapter-chatbot -- ollama pull <adapter-model>"
echo "  4. Test routing: ./scripts/test-adapters.sh"
echo "  5. Monitor resources: ./scripts/monitor.sh"
echo ""
echo "ğŸ”— Service Endpoints:"
echo "  Base:           ollama-base.llama3-multi-adapter:11434"
echo "  Chatbot:        ollama-chatbot.llama3-multi-adapter:11434"
echo "  Code:           ollama-code.llama3-multi-adapter:11434"
echo "  Summarization:  ollama-summarization.llama3-multi-adapter:11434"
echo ""
echo "ğŸŒ Ingress Paths (add to /etc/hosts: <node-ip> ollama.local):"
echo "  http://ollama.local/base"
echo "  http://ollama.local/chatbot"
echo "  http://ollama.local/code"
echo "  http://ollama.local/summarization"
